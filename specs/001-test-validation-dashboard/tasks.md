# Tasks: Enhanced Testing and Validation Reporting

**Input**: Design documents from `/specs/001-test-validation-dashboard/`
**Prerequisites**: plan.md (required), spec.md (required for user stories), research.md, data-model.md, contracts/

**Tests**: Tests are NOT requested in the feature specification. Test tasks are NOT included per SpecKit rules.

**Organization**: Tasks are grouped by user story to enable independent implementation and testing of each story.

## Format: `[ID] [P?] [Story] Description`

- **[P]**: Can run in parallel (different files, no dependencies)
- **[Story]**: Which user story this task belongs to (e.g., US1, US2, US3)
- Include exact file paths in descriptions

## Path Conventions

- **Multi-module Python project**: `proratio_utilities/`, `proratio_signals/`, `proratio_tradehub/`, `tests/` at repository root
- Paths follow existing project structure per plan.md

---

## Phase 1: Setup (Shared Infrastructure)

**Purpose**: Project initialization and basic structure for integration testing and validation storage

- [X] T001 Create integration test directory structure at tests/test_integration/
- [X] T002 [P] Create database schema at proratio_utilities/data/schema.sql from contract specification

---

## Phase 2: Foundational (Blocking Prerequisites)

**Purpose**: Core infrastructure that MUST be complete before ANY user story can be implemented

**‚ö†Ô∏è CRITICAL**: No user story work can begin until this phase is complete

- [X] T003 Implement ValidationRepository base class in proratio_utilities/data/validation_repository.py using SQLAlchemy Core
- [X] T004 Implement insert_validation_result() method in ValidationRepository with validation rules
- [X] T005 [P] Implement query_validation_results() method in ValidationRepository with filtering
- [X] T006 [P] Implement get_latest_validation() method in ValidationRepository
- [X] T007 [P] Implement get_validation_by_id() method in ValidationRepository
- [X] T008 [P] Implement count_validations() method in ValidationRepository
- [X] T009 Add database connection pooling configuration in ValidationRepository using Pydantic Settings
- [X] T010 Implement error handling for database connection failures in ValidationRepository (DatabaseConnectionError)
- [X] T011 [P] Implement error handling for database write failures in ValidationRepository (DatabaseWriteError)
- [X] T012 [P] Implement error handling for database query failures in ValidationRepository (DatabaseQueryError)
- [X] T013 Create integration test fixtures in tests/test_integration/conftest.py (in-memory SQLite, test storage directories)
- [X] T014 [P] Create mock market data fixture in tests/test_integration/conftest.py for signal generation tests
- [X] T015 Update Pydantic Settings in proratio_utilities/config/ to include VALIDATION_DB_URL environment variable

**Checkpoint**: Foundation ready - user story implementation can now begin in parallel

---

## Phase 3: User Story 1 - Integration Test Coverage for Data Pipeline (Priority: P1) üéØ MVP

**Goal**: Verify that the entire data flow‚Äîfrom collection through storage to loading‚Äîworks correctly end-to-end, so integration issues are caught before affecting trading strategies.

**Independent Test**: Run `pytest tests/test_integration/test_data_pipeline.py -v` and verify all data pipeline integration tests pass, confirming data flows correctly from collectors through storage to loaders.

### Implementation for User Story 1

- [X] T016 [P] [US1] Create test_data_collection_to_storage() integration test in tests/test_integration/test_data_pipeline.py
- [X] T017 [P] [US1] Create test_storage_to_loading() integration test in tests/test_integration/test_data_pipeline.py
- [X] T018 [P] [US1] Create test_concurrent_reads() integration test in tests/test_integration/test_data_pipeline.py
- [X] T019 [US1] Add data pipeline error handling test in tests/test_integration/test_data_pipeline.py (error during collection)
- [X] T020 [US1] Add logging verification to data pipeline integration tests
- [X] T021 [US1] Verify integration test suite runs in <5 minutes per SC-001

**Checkpoint**: At this point, User Story 1 should be fully functional - data pipeline integration tests run and pass independently

---

## Phase 4: User Story 2 - Integration Test Coverage for Signal-to-Trade Workflow (Priority: P1)

**Goal**: Verify that signals generated by the signal system correctly trigger and manage trades through the trade hub, ensuring the entire trading workflow operates as expected.

**Independent Test**: Run `pytest tests/test_integration/test_signal_to_trade.py -v` and verify all signal-to-trade integration tests pass, confirming signals create and manage trades correctly through the full lifecycle.

### Implementation for User Story 2

- [X] T022 [P] [US2] Create test_signal_to_trade_creation() integration test in tests/test_integration/test_signal_to_trade.py
- [X] T023 [P] [US2] Create test_trade_management_workflow() integration test in tests/test_integration/test_signal_to_trade.py
- [X] T024 [P] [US2] Create test_trade_closure() integration test in tests/test_integration/test_signal_to_trade.py
- [X] T025 [US2] Add signal orchestration to trade hub integration test in tests/test_integration/test_signal_to_trade.py
- [X] T026 [US2] Add error handling test for signal-to-trade workflow (invalid signal handling)
- [X] T027 [US2] Add logging verification to signal-to-trade integration tests
- [X] T028 [US2] Verify clear, actionable failure messages per FR-007 and SC-003

**Checkpoint**: At this point, User Stories 1 AND 2 should both work independently - complete integration test suite passes

---

## Phase 5: User Story 3 - Centralized Validation Results Storage (Priority: P2)

**Goal**: Store all backtest validation results in a centralized database to track strategy performance over time and compare different parameter configurations.

**Independent Test**: Run `python scripts/validate_backtest_results.py --strategy GridTrading`, verify results are stored in database, then query database to retrieve stored metrics with `SELECT * FROM validation_results ORDER BY timestamp DESC LIMIT 1;`

### Implementation for User Story 3

- [X] T029 [US3] Create database migration script in scripts/create_validation_schema.py that runs validation_result_schema.sql
- [X] T030 [US3] Add git commit hash capture function in scripts/validate_backtest_results.py (handles missing git gracefully)
- [X] T031 [US3] Modify scripts/validate_backtest_results.py to parse backtest metrics after validation
- [X] T032 [US3] Integrate ValidationRepository into scripts/validate_backtest_results.py for database writes
- [X] T033 [US3] Add database write operation to validation script with error handling (non-blocking if DB unavailable)
- [X] T034 [US3] Add success/failure logging for database writes in validation script
- [X] T035 [US3] Verify validation script overhead <5% per SC-004
- [X] T036 [US3] Test validation script with database unavailable (should continue with file-based validation)
- [X] T037 [US3] Verify query performance <2 seconds per SC-005

**Checkpoint**: All user stories should now be independently functional - validation results persist to database

---

## Phase 6: Polish & Cross-Cutting Concerns

**Purpose**: Improvements that affect multiple user stories

- [X] T038 [P] Update quickstart.md with actual setup commands and expected outputs
- [X] T039 [P] Update CLAUDE.md in repository root with integration testing patterns
- [X] T040 [P] Create example query scripts in scripts/ for common validation result queries
- [X] T041 Code review for type hints and docstrings (Google-style) across all new files
- [X] T042 [P] Run Ruff linting on all new code: `ruff check proratio_utilities/data/ scripts/ tests/test_integration/`
- [X] T043 [P] Run Ruff formatting on all new code: `ruff format proratio_utilities/data/ scripts/ tests/test_integration/`
- [X] T044 Verify all database credentials are in .env and not hardcoded per constitution
- [X] T045 [P] Run integration test suite end-to-end and verify <5 minute completion time (SC-001)
- [X] T046 Update specs/001-test-validation-dashboard/quickstart.md based on implementation learnings

---

## Dependencies & Execution Order

### Phase Dependencies

- **Setup (Phase 1)**: No dependencies - can start immediately
- **Foundational (Phase 2)**: Depends on Setup completion - BLOCKS all user stories
- **User Stories (Phase 3+)**: All depend on Foundational phase completion
  - User stories can then proceed in parallel (if staffed)
  - Or sequentially in priority order (US1 ‚Üí US2 ‚Üí US3)
- **Polish (Phase 6)**: Depends on all desired user stories being complete

### User Story Dependencies

- **User Story 1 (P1)**: Can start after Foundational (Phase 2) - No dependencies on other stories
- **User Story 2 (P1)**: Can start after Foundational (Phase 2) - No dependencies on User Story 1 (both are P1)
- **User Story 3 (P2)**: Can start after Foundational (Phase 2) - No dependencies on User Stories 1 or 2

### Within Each User Story

- Models before services (N/A for this feature - no new models)
- Services before endpoints (N/A - no endpoints)
- Core implementation before integration
- Story complete before moving to next priority

### Parallel Opportunities

- **Phase 1 (Setup)**: T001 and T002 can run in parallel (different files)
- **Phase 2 (Foundational)**: T005-T008 can run in parallel after T004 completes (different methods in same class)
- **Phase 2 (Foundational)**: T010-T012 can run in parallel (different error handlers)
- **Phase 2 (Foundational)**: T013 and T014 can run separately (different fixtures)
- **Phase 3 (US1)**: T016-T018 can run in parallel (different test functions)
- **Phase 4 (US2)**: T022-T024 can run in parallel (different test functions)
- **Phase 6 (Polish)**: T038-T040, T042-T043, T045 can run in parallel (different files/commands)
- Once Foundational phase completes:
  - User Story 1 (Phase 3) and User Story 2 (Phase 4) can be implemented in parallel by different developers
  - User Story 3 (Phase 5) can start independently after Foundational phase

---

## Parallel Example: User Story 1

```bash
# Launch all integration test implementations for User Story 1 together:
Task: "[US1] Create test_data_collection_to_storage() integration test in tests/test_integration/test_data_pipeline.py"
Task: "[US1] Create test_storage_to_loading() integration test in tests/test_integration/test_data_pipeline.py"
Task: "[US1] Create test_concurrent_reads() integration test in tests/test_integration/test_data_pipeline.py"
```

## Parallel Example: User Story 2

```bash
# Launch all integration test implementations for User Story 2 together:
Task: "[US2] Create test_signal_to_trade_creation() integration test in tests/test_integration/test_signal_to_trade.py"
Task: "[US2] Create test_trade_management_workflow() integration test in tests/test_integration/test_signal_to_trade.py"
Task: "[US2] Create test_trade_closure() integration test in tests/test_integration/test_signal_to_trade.py"
```

## Parallel Example: Foundational Phase

```bash
# After T003 and T004 complete, launch repository methods together:
Task: "Implement query_validation_results() method in ValidationRepository with filtering"
Task: "Implement get_latest_validation() method in ValidationRepository"
Task: "Implement get_validation_by_id() method in ValidationRepository"
Task: "Implement count_validations() method in ValidationRepository"
```

---

## Implementation Strategy

### MVP First (User Stories 1 & 2 - Both P1)

1. Complete Phase 1: Setup (T001-T002)
2. Complete Phase 2: Foundational (T003-T015) - CRITICAL: blocks all stories
3. Complete Phase 3: User Story 1 (T016-T021)
4. **STOP and VALIDATE**: Run `pytest tests/test_integration/test_data_pipeline.py -v` to verify data pipeline tests pass independently
5. Complete Phase 4: User Story 2 (T022-T028)
6. **STOP and VALIDATE**: Run `pytest tests/test_integration/ -v` to verify all integration tests pass
7. Deploy/demo if ready (integration test framework is now operational)

### Incremental Delivery

1. Complete Setup + Foundational ‚Üí Foundation ready (ValidationRepository works)
2. Add User Story 1 ‚Üí Test independently ‚Üí Integration tests for data pipeline functional (MVP milestone 1!)
3. Add User Story 2 ‚Üí Test independently ‚Üí Complete integration test suite operational (MVP milestone 2!)
4. Add User Story 3 ‚Üí Test independently ‚Üí Validation results now persist to database (P2 feature added!)
5. Each story adds value without breaking previous stories

### Parallel Team Strategy

With multiple developers:

1. Team completes Setup + Foundational together (T001-T015)
2. Once Foundational is done:
   - Developer A: User Story 1 (T016-T021) - Data pipeline integration tests
   - Developer B: User Story 2 (T022-T028) - Signal-to-trade integration tests
   - Developer C: User Story 3 (T029-T037) - Validation storage (can start immediately after foundational)
3. Stories complete and integrate independently
4. Team reconvenes for Polish phase (T038-T046)

---

## Success Metrics (from spec.md)

Implementation will be considered complete when:

- ‚úÖ **SC-001**: Integration test suite runs in <5 minutes with clear pass/fail output (verified at T021, T028, T045)
- ‚úÖ **SC-002**: 100% of backtest validation runs successfully store results in database (verified at T033, T036)
- ‚úÖ **SC-003**: Integration test failures clearly identify failing component (verified at T028)
- ‚úÖ **SC-004**: Validation script overhead is <5% after adding database writes (verified at T035)
- ‚úÖ **SC-005**: Querying validation results returns data in <2 seconds (verified at T037)
- ‚úÖ **SC-006**: Integration tests catch 90%+ of integration issues (measured over time after deployment)

---

## Notes

- [P] tasks = different files or different methods, no dependencies
- [Story] label maps task to specific user story for traceability
- Each user story should be independently completable and testable
- Tests are NOT included in this feature (no TDD requested in spec)
- Commit after each task or logical group
- Stop at any checkpoint to validate story independently
- Avoid: vague tasks, same file conflicts, cross-story dependencies that break independence
- **No test writing tasks**: Per spec, this feature IS the testing infrastructure - integration tests themselves verify the implementation works
- **Database setup**: Run `specs/001-test-validation-dashboard/contracts/validation_result_schema.sql` manually before Phase 5 (User Story 3)
- **Environment variables**: Ensure `VALIDATION_DB_URL` is in `.env` before starting Phase 5

---

## Task Count Summary

- **Total Tasks**: 46
- **Phase 1 (Setup)**: 2 tasks (T001-T002)
- **Phase 2 (Foundational)**: 13 tasks (T003-T015)
- **Phase 3 (User Story 1)**: 6 tasks (T016-T021)
- **Phase 4 (User Story 2)**: 7 tasks (T022-T028)
- **Phase 5 (User Story 3)**: 9 tasks (T029-T037)
- **Phase 6 (Polish)**: 9 tasks (T038-T046)

**Parallel Opportunities Identified**: 20 tasks marked [P] can run in parallel with other tasks in their phase

**MVP Scope**: Phase 1 + Phase 2 + Phase 3 + Phase 4 = **28 tasks** (User Stories 1 & 2, both P1 priority)

**Independent Test Criteria**:
- **User Story 1**: `pytest tests/test_integration/test_data_pipeline.py -v` ‚Üí all tests pass
- **User Story 2**: `pytest tests/test_integration/test_signal_to_trade.py -v` ‚Üí all tests pass
- **User Story 3**: Run validation script, verify database contains results

---

## Format Validation

‚úÖ **ALL tasks follow the required checklist format**:
- Every task starts with `- [ ]` (markdown checkbox)
- Every task has sequential Task ID (T001, T002, T003...)
- Tasks include [P] marker where parallelizable
- User story phase tasks include [Story] label (US1, US2, US3)
- Every task has clear description with exact file path
- Setup and Foundational phases have no [Story] label (correct per rules)
